{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-22 15:24:26--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1.06M   963KB/s    in 1.1s    \n",
      "\n",
      "2024-05-22 15:24:28 (963 KB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "with open('input.txt','r',encoding='UTF-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n",
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# build vocab, encoder and decoder\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n",
    "\n",
    "stoi = {c:i for i,c in enumerate(chars)}\n",
    "itos = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda v: ''.join(itos[vv] for vv in v)\n",
    "\n",
    "print(encode('hii there'))\n",
    "print(decode(encode('hii there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into train, validation and test datasets\n",
    "\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[0:n]\n",
    "validation_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the context is tensor([18]), the target is 47\n",
      "When the context is tensor([18, 47]), the target is 56\n",
      "When the context is tensor([18, 47, 56]), the target is 57\n",
      "When the context is tensor([18, 47, 56, 57]), the target is 58\n",
      "When the context is tensor([18, 47, 56, 57, 58]), the target is 1\n",
      "When the context is tensor([18, 47, 56, 57, 58,  1]), the target is 15\n",
      "When the context is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47\n",
      "When the context is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "\n",
    "    print(f\"When the context is {context}, the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "---------------\n",
      "When the context is tensor([24]), the target is 43\n",
      "When the context is tensor([24, 43]), the target is 58\n",
      "When the context is tensor([24, 43, 58]), the target is 5\n",
      "When the context is tensor([24, 43, 58,  5]), the target is 57\n",
      "When the context is tensor([24, 43, 58,  5, 57]), the target is 1\n",
      "When the context is tensor([24, 43, 58,  5, 57,  1]), the target is 46\n",
      "When the context is tensor([24, 43, 58,  5, 57,  1, 46]), the target is 43\n",
      "When the context is tensor([24, 43, 58,  5, 57,  1, 46, 43]), the target is 39\n",
      "When the context is tensor([44]), the target is 53\n",
      "When the context is tensor([44, 53]), the target is 56\n",
      "When the context is tensor([44, 53, 56]), the target is 1\n",
      "When the context is tensor([44, 53, 56,  1]), the target is 58\n",
      "When the context is tensor([44, 53, 56,  1, 58]), the target is 46\n",
      "When the context is tensor([44, 53, 56,  1, 58, 46]), the target is 39\n",
      "When the context is tensor([44, 53, 56,  1, 58, 46, 39]), the target is 58\n",
      "When the context is tensor([44, 53, 56,  1, 58, 46, 39, 58]), the target is 1\n",
      "When the context is tensor([52]), the target is 58\n",
      "When the context is tensor([52, 58]), the target is 1\n",
      "When the context is tensor([52, 58,  1]), the target is 58\n",
      "When the context is tensor([52, 58,  1, 58]), the target is 46\n",
      "When the context is tensor([52, 58,  1, 58, 46]), the target is 39\n",
      "When the context is tensor([52, 58,  1, 58, 46, 39]), the target is 58\n",
      "When the context is tensor([52, 58,  1, 58, 46, 39, 58]), the target is 1\n",
      "When the context is tensor([52, 58,  1, 58, 46, 39, 58,  1]), the target is 46\n",
      "When the context is tensor([25]), the target is 17\n",
      "When the context is tensor([25, 17]), the target is 27\n",
      "When the context is tensor([25, 17, 27]), the target is 10\n",
      "When the context is tensor([25, 17, 27, 10]), the target is 0\n",
      "When the context is tensor([25, 17, 27, 10,  0]), the target is 21\n",
      "When the context is tensor([25, 17, 27, 10,  0, 21]), the target is 1\n",
      "When the context is tensor([25, 17, 27, 10,  0, 21,  1]), the target is 54\n",
      "When the context is tensor([25, 17, 27, 10,  0, 21,  1, 54]), the target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else validation_data\n",
    "    ix = torch.randint(0,len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('---------------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b,:t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"When the context is {context}, the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targerts are both of shape (batch_size, block_size) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # shape: (batch_size, block_size, vocab_size) or (B, T, C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshaping logits so that we can compute the loss\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            # predicting the loss through cross entropy\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    # Sample from the distribution through the model \n",
    "    def generate(self, idx, max_new_tokens=100):\n",
    "        with torch.no_grad():\n",
    "            for t in range(max_new_tokens):\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx)\n",
    "                # Use only the last step\n",
    "                logits = logits[:, -1, :] # for all batches, take the last token\n",
    "                # get the probability distribution\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                # sample from the distribution\n",
    "                sample = torch.multinomial(probs, num_samples=1)\n",
    "                # append to the sequence\n",
    "                idx = torch.cat((idx, sample.view(-1, 1)), dim=1)\n",
    "        return idx\n",
    "\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "tks = m.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "print(decode(tks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3436813354492188\n",
      "2.390263795852661\n",
      "2.534330368041992\n",
      "2.4038283824920654\n",
      "2.603792905807495\n",
      "2.4936015605926514\n",
      "2.4469120502471924\n",
      "2.4720141887664795\n",
      "2.50858473777771\n",
      "2.563898801803589\n",
      "2.516644239425659\n",
      "2.534618377685547\n",
      "2.450871706008911\n",
      "2.5475900173187256\n",
      "2.5122485160827637\n",
      "2.3961195945739746\n",
      "2.5305325984954834\n",
      "2.547184705734253\n",
      "2.5595877170562744\n",
      "2.5234274864196777\n",
      "2.5520293712615967\n",
      "2.41968035697937\n",
      "2.5986216068267822\n",
      "2.527085542678833\n",
      "2.6083054542541504\n",
      "2.469792127609253\n",
      "2.4987616539001465\n",
      "2.483426332473755\n",
      "2.6692516803741455\n",
      "2.5700106620788574\n",
      "2.568403482437134\n",
      "2.635690689086914\n",
      "2.3716225624084473\n",
      "2.553765296936035\n",
      "2.524486780166626\n",
      "2.4647772312164307\n",
      "2.5398011207580566\n",
      "2.530543327331543\n",
      "2.4723949432373047\n",
      "2.443235158920288\n",
      "2.4367730617523193\n",
      "2.369100332260132\n",
      "2.494701862335205\n",
      "2.363583564758301\n",
      "2.43719744682312\n",
      "2.6231653690338135\n",
      "2.4482808113098145\n",
      "2.380394220352173\n",
      "2.5506653785705566\n",
      "2.4480056762695312\n",
      "2.4132583141326904\n",
      "2.436281681060791\n",
      "2.447112560272217\n",
      "2.566087007522583\n",
      "2.4601261615753174\n",
      "2.506688356399536\n",
      "2.513258218765259\n",
      "2.3545727729797363\n",
      "2.544267177581787\n",
      "2.407299280166626\n",
      "2.5013294219970703\n",
      "2.6433823108673096\n",
      "2.415349006652832\n",
      "2.5080020427703857\n",
      "2.416248321533203\n",
      "2.525890588760376\n",
      "2.4637837409973145\n",
      "2.62733793258667\n",
      "2.4117021560668945\n",
      "2.625710964202881\n",
      "2.5821540355682373\n",
      "2.3386902809143066\n",
      "2.467735767364502\n",
      "2.4718990325927734\n",
      "2.6625757217407227\n",
      "2.5404977798461914\n",
      "2.5162174701690674\n",
      "2.4789836406707764\n",
      "2.45784592628479\n",
      "2.4944252967834473\n",
      "2.6101226806640625\n",
      "2.402991533279419\n",
      "2.6223790645599365\n",
      "2.583674192428589\n",
      "2.553422212600708\n",
      "2.477351665496826\n",
      "2.5658252239227295\n",
      "2.3522768020629883\n",
      "2.3556244373321533\n",
      "2.4685215950012207\n",
      "2.485008955001831\n",
      "2.4385294914245605\n",
      "2.4728493690490723\n",
      "2.504324436187744\n",
      "2.42702317237854\n",
      "2.3649046421051025\n",
      "2.492856502532959\n",
      "2.441056251525879\n",
      "2.583089828491211\n",
      "2.522789239883423\n",
      "2.362419843673706\n",
      "2.5264861583709717\n",
      "2.5929665565490723\n",
      "2.445178270339966\n",
      "2.382572650909424\n",
      "2.2902157306671143\n",
      "2.394110918045044\n",
      "2.402296781539917\n",
      "2.631150484085083\n",
      "2.5012142658233643\n",
      "2.5644285678863525\n",
      "2.4452455043792725\n",
      "2.432159185409546\n",
      "2.336712598800659\n",
      "2.588268995285034\n",
      "2.3389241695404053\n",
      "2.543200969696045\n",
      "2.638180732727051\n",
      "2.6677708625793457\n",
      "2.5154261589050293\n",
      "2.312617301940918\n",
      "2.54858136177063\n",
      "2.48018741607666\n",
      "2.424535036087036\n",
      "2.398244619369507\n",
      "2.462658643722534\n",
      "2.484734058380127\n",
      "2.3630757331848145\n",
      "2.3232667446136475\n",
      "2.4955966472625732\n",
      "2.5553300380706787\n",
      "2.3868038654327393\n",
      "2.487157106399536\n",
      "2.461169481277466\n",
      "2.5036847591400146\n",
      "2.435034990310669\n",
      "2.4739882946014404\n",
      "2.4349591732025146\n",
      "2.5156378746032715\n",
      "2.4869065284729004\n",
      "2.5305566787719727\n",
      "2.4759860038757324\n",
      "2.4755783081054688\n",
      "2.575047731399536\n",
      "2.479929208755493\n",
      "2.3393421173095703\n",
      "2.4792709350585938\n",
      "2.4687716960906982\n",
      "2.410166025161743\n",
      "2.441270112991333\n",
      "2.5763602256774902\n",
      "2.358424186706543\n",
      "2.4640004634857178\n",
      "2.61484432220459\n",
      "2.407569408416748\n",
      "2.580026626586914\n",
      "2.428760528564453\n",
      "2.5599186420440674\n",
      "2.3816637992858887\n",
      "2.568295478820801\n",
      "2.440674304962158\n",
      "2.59169602394104\n",
      "2.5665597915649414\n",
      "2.4693775177001953\n",
      "2.4667556285858154\n",
      "2.48846173286438\n",
      "2.529993772506714\n",
      "2.3701329231262207\n",
      "2.4695651531219482\n",
      "2.572021484375\n",
      "2.4337375164031982\n",
      "2.3797154426574707\n",
      "2.4849565029144287\n",
      "2.4397575855255127\n",
      "2.4876723289489746\n",
      "2.462841272354126\n",
      "2.51600980758667\n",
      "2.4650580883026123\n",
      "2.3610622882843018\n",
      "2.5220203399658203\n",
      "2.648707866668701\n",
      "2.5087807178497314\n",
      "2.553590774536133\n",
      "2.3548126220703125\n",
      "2.4743456840515137\n",
      "2.382439136505127\n",
      "2.4343645572662354\n",
      "2.3680999279022217\n",
      "2.5537869930267334\n",
      "2.4605300426483154\n",
      "2.4874417781829834\n",
      "2.3841183185577393\n",
      "2.516660690307617\n",
      "2.470917224884033\n",
      "2.5982487201690674\n",
      "2.5127289295196533\n",
      "2.4753055572509766\n",
      "2.5111756324768066\n",
      "2.4049644470214844\n",
      "2.5169363021850586\n",
      "2.3859870433807373\n",
      "2.385856866836548\n",
      "2.429544448852539\n",
      "2.295830726623535\n",
      "2.5686848163604736\n",
      "2.5356087684631348\n",
      "2.423349380493164\n",
      "2.469132423400879\n",
      "2.584524393081665\n",
      "2.501633882522583\n",
      "2.378416061401367\n",
      "2.3842058181762695\n",
      "2.4383747577667236\n",
      "2.411362648010254\n",
      "2.466905117034912\n",
      "2.395866870880127\n",
      "2.510504722595215\n",
      "2.5168368816375732\n",
      "2.3731870651245117\n",
      "2.57121205329895\n",
      "2.598116874694824\n",
      "2.5427136421203613\n",
      "2.477060079574585\n",
      "2.4878716468811035\n",
      "2.496438503265381\n",
      "2.4731411933898926\n",
      "2.5591800212860107\n",
      "2.499760866165161\n",
      "2.4047279357910156\n",
      "2.2352941036224365\n",
      "2.4652421474456787\n",
      "2.481827735900879\n",
      "2.4746501445770264\n",
      "2.583906650543213\n",
      "2.4151785373687744\n",
      "2.5262019634246826\n",
      "2.3785245418548584\n",
      "2.4611167907714844\n",
      "2.3674261569976807\n",
      "2.4918324947357178\n",
      "2.4303653240203857\n",
      "2.360191822052002\n",
      "2.540039300918579\n",
      "2.5835819244384766\n",
      "2.5598456859588623\n",
      "2.5517258644104004\n",
      "2.6183714866638184\n",
      "2.565145492553711\n",
      "2.4392147064208984\n",
      "2.4456984996795654\n",
      "2.416365146636963\n",
      "2.6601245403289795\n",
      "2.4384636878967285\n",
      "2.3100483417510986\n",
      "2.393292188644409\n",
      "2.449751377105713\n",
      "2.3875741958618164\n",
      "2.4584758281707764\n",
      "2.4380242824554443\n",
      "2.5093300342559814\n",
      "2.4358527660369873\n",
      "2.5241382122039795\n",
      "2.596195936203003\n",
      "2.553243398666382\n",
      "2.550076723098755\n",
      "2.3908872604370117\n",
      "2.489079475402832\n",
      "2.344984531402588\n",
      "2.720470428466797\n",
      "2.381894826889038\n",
      "2.35353946685791\n",
      "2.4806551933288574\n",
      "2.6042428016662598\n",
      "2.375910520553589\n",
      "2.4565253257751465\n",
      "2.5562055110931396\n",
      "2.5121874809265137\n",
      "2.518860101699829\n",
      "2.3852078914642334\n",
      "2.3550305366516113\n",
      "2.4568564891815186\n",
      "2.4335875511169434\n",
      "2.4017553329467773\n",
      "2.4795665740966797\n",
      "2.5672554969787598\n",
      "2.3536460399627686\n",
      "2.5186777114868164\n",
      "2.4560492038726807\n",
      "2.6183061599731445\n",
      "2.4297289848327637\n",
      "2.4704599380493164\n",
      "2.431103467941284\n",
      "2.4523212909698486\n",
      "2.515963554382324\n",
      "2.375807762145996\n",
      "2.419919490814209\n",
      "2.4850401878356934\n",
      "2.402024984359741\n",
      "2.3530097007751465\n",
      "2.3914718627929688\n",
      "2.505725622177124\n",
      "2.4459495544433594\n",
      "2.5325865745544434\n",
      "2.5806009769439697\n",
      "2.5423312187194824\n",
      "2.4102797508239746\n",
      "2.436743974685669\n",
      "2.5121304988861084\n",
      "2.414294958114624\n",
      "2.425858736038208\n",
      "2.448336601257324\n",
      "2.473148822784424\n",
      "2.403590679168701\n",
      "2.5472679138183594\n",
      "2.433321952819824\n",
      "2.4373984336853027\n",
      "2.3419928550720215\n",
      "2.3619723320007324\n",
      "2.407419443130493\n",
      "2.4184813499450684\n",
      "2.4887492656707764\n",
      "2.3502871990203857\n",
      "2.504694700241089\n",
      "2.618299961090088\n",
      "2.4307944774627686\n",
      "2.3674368858337402\n",
      "2.3992135524749756\n",
      "2.504373073577881\n",
      "2.3867275714874268\n",
      "2.3364949226379395\n",
      "2.587235927581787\n",
      "2.476161241531372\n",
      "2.528306484222412\n",
      "2.4888150691986084\n",
      "2.5486083030700684\n",
      "2.463263988494873\n",
      "2.458413600921631\n",
      "2.514843702316284\n",
      "2.3724489212036133\n",
      "2.6299664974212646\n",
      "2.5074098110198975\n",
      "2.5155117511749268\n",
      "2.365990161895752\n",
      "2.5184855461120605\n",
      "2.3286378383636475\n",
      "2.4678423404693604\n",
      "2.50559401512146\n",
      "2.536977529525757\n",
      "2.420874834060669\n",
      "2.3823556900024414\n",
      "2.4595274925231934\n",
      "2.3408823013305664\n",
      "2.550703763961792\n",
      "2.4128448963165283\n",
      "2.474705696105957\n",
      "2.3994693756103516\n",
      "2.4996352195739746\n",
      "2.4857864379882812\n",
      "2.467585325241089\n",
      "2.564883232116699\n",
      "2.6008689403533936\n",
      "2.391451597213745\n",
      "2.468266487121582\n",
      "2.468332290649414\n",
      "2.451524496078491\n",
      "2.4145610332489014\n",
      "2.504749298095703\n",
      "2.5181097984313965\n",
      "2.4607489109039307\n",
      "2.536186456680298\n",
      "2.354186534881592\n",
      "2.5069143772125244\n",
      "2.245643377304077\n",
      "2.5904877185821533\n",
      "2.4090144634246826\n",
      "2.4710211753845215\n",
      "2.383387804031372\n",
      "2.4278669357299805\n",
      "2.4353671073913574\n",
      "2.405510187149048\n",
      "2.39731764793396\n",
      "2.364485025405884\n",
      "2.537858724594116\n",
      "2.4454190731048584\n",
      "2.471560478210449\n",
      "2.359494924545288\n",
      "2.3492891788482666\n",
      "2.5110912322998047\n",
      "2.36972713470459\n",
      "2.6769959926605225\n",
      "2.468336820602417\n",
      "2.557152271270752\n",
      "2.481093406677246\n",
      "2.412745714187622\n",
      "2.3724772930145264\n",
      "2.4988934993743896\n",
      "2.451385021209717\n",
      "2.316229820251465\n",
      "2.583613395690918\n",
      "2.3446996212005615\n",
      "2.4248485565185547\n",
      "2.4515604972839355\n",
      "2.489736795425415\n",
      "2.4702069759368896\n",
      "2.521239757537842\n",
      "2.4876437187194824\n",
      "2.3569586277008057\n",
      "2.53204345703125\n",
      "2.386935234069824\n",
      "2.689657688140869\n",
      "2.5212690830230713\n",
      "2.484794855117798\n",
      "2.4861276149749756\n",
      "2.5446157455444336\n",
      "2.406118869781494\n",
      "2.406503677368164\n",
      "2.385246992111206\n",
      "2.421438455581665\n",
      "2.497562885284424\n",
      "2.518455743789673\n",
      "2.4515981674194336\n",
      "2.5761733055114746\n",
      "2.4725282192230225\n",
      "2.5151569843292236\n",
      "2.4614651203155518\n",
      "2.458388566970825\n",
      "2.4348201751708984\n",
      "2.549863576889038\n",
      "2.500300884246826\n",
      "2.444064140319824\n",
      "2.512019395828247\n",
      "2.38718843460083\n",
      "2.7639994621276855\n",
      "2.6338858604431152\n",
      "2.4703097343444824\n",
      "2.4695072174072266\n",
      "2.5020194053649902\n",
      "2.454864263534546\n",
      "2.568552017211914\n",
      "2.324326276779175\n",
      "2.4265029430389404\n",
      "2.4877920150756836\n",
      "2.4421615600585938\n",
      "2.4278666973114014\n",
      "2.5855531692504883\n",
      "2.459340810775757\n",
      "2.4766767024993896\n",
      "2.4437408447265625\n",
      "2.5027859210968018\n",
      "2.365542411804199\n",
      "2.3935048580169678\n",
      "2.412170171737671\n",
      "2.5699541568756104\n",
      "2.384016752243042\n",
      "2.446587085723877\n",
      "2.5042190551757812\n",
      "2.6622159481048584\n",
      "2.443325996398926\n",
      "2.303058385848999\n",
      "2.3519978523254395\n",
      "2.477163076400757\n",
      "2.437617063522339\n",
      "2.499685287475586\n",
      "2.4747540950775146\n",
      "2.465899705886841\n",
      "2.536879539489746\n",
      "2.4406497478485107\n",
      "2.459242582321167\n",
      "2.365865468978882\n",
      "2.544363260269165\n",
      "2.389185667037964\n",
      "2.4297263622283936\n",
      "2.4934258460998535\n",
      "2.494037628173828\n",
      "2.480508327484131\n",
      "2.4283506870269775\n",
      "2.6024112701416016\n",
      "2.5994927883148193\n",
      "2.5136444568634033\n",
      "2.4719955921173096\n",
      "2.5379114151000977\n",
      "2.4384877681732178\n",
      "2.423220157623291\n",
      "2.415670871734619\n",
      "2.5087480545043945\n",
      "2.610788583755493\n",
      "2.544635057449341\n",
      "2.4704790115356445\n",
      "2.317190408706665\n",
      "2.3010079860687256\n",
      "2.3932559490203857\n",
      "2.352680206298828\n",
      "2.478294610977173\n",
      "2.4111275672912598\n",
      "2.5076444149017334\n",
      "2.451620578765869\n",
      "2.535285234451294\n",
      "2.541480302810669\n",
      "2.439934015274048\n",
      "2.503366708755493\n",
      "2.519122362136841\n",
      "2.40063214302063\n",
      "2.454984426498413\n",
      "2.6016411781311035\n",
      "2.401350975036621\n",
      "2.4742813110351562\n",
      "2.554037094116211\n",
      "2.463517189025879\n",
      "2.5618274211883545\n",
      "2.3512418270111084\n",
      "2.4262747764587402\n",
      "2.5259196758270264\n",
      "2.5273001194000244\n",
      "2.598198652267456\n",
      "2.5156941413879395\n",
      "2.4338061809539795\n",
      "2.568209409713745\n",
      "2.3351175785064697\n",
      "2.4700982570648193\n",
      "2.356112003326416\n",
      "2.5344600677490234\n",
      "2.372295618057251\n",
      "2.3485896587371826\n",
      "2.468916893005371\n",
      "2.429912567138672\n",
      "2.389718770980835\n",
      "2.4944543838500977\n",
      "2.4938437938690186\n",
      "2.608081340789795\n",
      "2.4312455654144287\n",
      "2.430593252182007\n",
      "2.707378387451172\n",
      "2.3124444484710693\n",
      "2.6021077632904053\n",
      "2.464045286178589\n",
      "2.447882890701294\n",
      "2.486572265625\n",
      "2.3940842151641846\n",
      "2.3971405029296875\n",
      "2.5809929370880127\n",
      "2.3816936016082764\n",
      "2.3740057945251465\n",
      "2.466475248336792\n",
      "2.5899970531463623\n",
      "2.5325655937194824\n",
      "2.3101818561553955\n",
      "2.454134702682495\n",
      "2.408478021621704\n",
      "2.5374932289123535\n",
      "2.4561989307403564\n",
      "2.479458808898926\n",
      "2.5153777599334717\n",
      "2.490217924118042\n",
      "2.464221715927124\n",
      "2.438777208328247\n",
      "2.440760850906372\n",
      "2.561434507369995\n",
      "2.477128505706787\n",
      "2.4890425205230713\n",
      "2.566105842590332\n",
      "2.581183671951294\n",
      "2.4690120220184326\n",
      "2.6163065433502197\n",
      "2.510648727416992\n",
      "2.369952917098999\n",
      "2.4610862731933594\n",
      "2.4614784717559814\n",
      "2.443643808364868\n",
      "2.3846700191497803\n",
      "2.3231732845306396\n",
      "2.4342591762542725\n",
      "2.416879415512085\n",
      "2.655083417892456\n",
      "2.428407669067383\n",
      "2.41479229927063\n",
      "2.5534486770629883\n",
      "2.409240484237671\n",
      "2.4835495948791504\n",
      "2.468531847000122\n",
      "2.4780526161193848\n",
      "2.416752338409424\n",
      "2.629499673843384\n",
      "2.4524426460266113\n",
      "2.425966262817383\n",
      "2.5291764736175537\n",
      "2.49053692817688\n",
      "2.420562267303467\n",
      "2.3472800254821777\n",
      "2.448493003845215\n",
      "2.4692134857177734\n",
      "2.4251186847686768\n",
      "2.5538330078125\n",
      "2.454312562942505\n",
      "2.4923722743988037\n",
      "2.4192724227905273\n",
      "2.36087965965271\n",
      "2.4264333248138428\n",
      "2.531442403793335\n",
      "2.2700018882751465\n",
      "2.367661237716675\n",
      "2.332540273666382\n",
      "2.334428548812866\n",
      "2.5299432277679443\n",
      "2.530273675918579\n",
      "2.3941426277160645\n",
      "2.4788129329681396\n",
      "2.553105592727661\n",
      "2.532625198364258\n",
      "2.678804636001587\n",
      "2.4622983932495117\n",
      "2.685045003890991\n",
      "2.481537103652954\n",
      "2.4838030338287354\n",
      "2.58805251121521\n",
      "2.439419984817505\n",
      "2.6014022827148438\n",
      "2.489968776702881\n",
      "2.377110242843628\n",
      "2.6589062213897705\n",
      "2.469118118286133\n",
      "2.433936357498169\n",
      "2.415717363357544\n",
      "2.439619302749634\n",
      "2.4454076290130615\n",
      "2.535219192504883\n",
      "2.439147472381592\n",
      "2.5078623294830322\n",
      "2.4447672367095947\n",
      "2.4628069400787354\n",
      "2.5181331634521484\n",
      "2.473410129547119\n",
      "2.438628673553467\n",
      "2.4225480556488037\n",
      "2.4904353618621826\n",
      "2.389631509780884\n",
      "2.588501453399658\n",
      "2.5092878341674805\n",
      "2.4735894203186035\n",
      "2.540917158126831\n",
      "2.3500540256500244\n",
      "2.3213021755218506\n",
      "2.4399845600128174\n",
      "2.3817617893218994\n",
      "2.6447463035583496\n",
      "2.429807424545288\n",
      "2.3620429039001465\n",
      "2.400223970413208\n",
      "2.539625883102417\n",
      "2.6611037254333496\n",
      "2.4547080993652344\n",
      "2.345665693283081\n",
      "2.5655412673950195\n",
      "2.532607316970825\n",
      "2.46360445022583\n",
      "2.447716474533081\n",
      "2.488429307937622\n",
      "2.408592700958252\n",
      "2.5029640197753906\n",
      "2.3716888427734375\n",
      "2.4341461658477783\n",
      "2.5080506801605225\n",
      "2.41037654876709\n",
      "2.5065157413482666\n",
      "2.492326021194458\n",
      "2.523033618927002\n",
      "2.249405860900879\n",
      "2.5151777267456055\n",
      "2.3160998821258545\n",
      "2.5615761280059814\n",
      "2.4803359508514404\n",
      "2.573101282119751\n",
      "2.552395820617676\n",
      "2.522435188293457\n",
      "2.4215574264526367\n",
      "2.367368459701538\n",
      "2.451376438140869\n",
      "2.336106777191162\n",
      "2.4874672889709473\n",
      "2.515392303466797\n",
      "2.4133944511413574\n",
      "2.547213077545166\n",
      "2.5200953483581543\n",
      "2.3064050674438477\n",
      "2.490847587585449\n",
      "2.4619815349578857\n",
      "2.669811248779297\n",
      "2.582026243209839\n",
      "2.5539159774780273\n",
      "2.6166462898254395\n",
      "2.5016274452209473\n",
      "2.4239301681518555\n",
      "2.4824962615966797\n",
      "2.452648401260376\n",
      "2.628911018371582\n",
      "2.340456962585449\n",
      "2.4395997524261475\n",
      "2.5520968437194824\n",
      "2.3992412090301514\n",
      "2.420397996902466\n",
      "2.5366690158843994\n",
      "2.4187703132629395\n",
      "2.4061079025268555\n",
      "2.4760055541992188\n",
      "2.3910772800445557\n",
      "2.525698661804199\n",
      "2.4775683879852295\n",
      "2.3836731910705566\n",
      "2.4469010829925537\n",
      "2.5303311347961426\n",
      "2.3779044151306152\n",
      "2.4677040576934814\n",
      "2.419957160949707\n",
      "2.5306360721588135\n",
      "2.570317029953003\n",
      "2.4351234436035156\n",
      "2.39608097076416\n",
      "2.3899331092834473\n",
      "2.3137850761413574\n",
      "2.6982147693634033\n",
      "2.470532178878784\n",
      "2.49180269241333\n",
      "2.620753288269043\n",
      "2.288177967071533\n",
      "2.415900230407715\n",
      "2.47583270072937\n",
      "2.6927762031555176\n",
      "2.637345314025879\n",
      "2.482295513153076\n",
      "2.5148956775665283\n",
      "2.5559914112091064\n",
      "2.468867063522339\n",
      "2.5103330612182617\n",
      "2.4764561653137207\n",
      "2.42607045173645\n",
      "2.700650453567505\n",
      "2.472597122192383\n",
      "2.4505515098571777\n",
      "2.3783557415008545\n",
      "2.387773275375366\n",
      "2.508939743041992\n",
      "2.4641053676605225\n",
      "2.6161558628082275\n",
      "2.395418405532837\n",
      "2.608505964279175\n",
      "2.469794988632202\n",
      "2.5494208335876465\n",
      "2.3938841819763184\n",
      "2.6164772510528564\n",
      "2.4274652004241943\n",
      "2.4959845542907715\n",
      "2.4308207035064697\n",
      "2.515782117843628\n",
      "2.5338518619537354\n",
      "2.475367307662964\n",
      "2.557044267654419\n",
      "2.7341103553771973\n",
      "2.440147876739502\n",
      "2.501215696334839\n",
      "2.5997724533081055\n",
      "2.578526258468628\n",
      "2.530332565307617\n",
      "2.4130353927612305\n",
      "2.5489401817321777\n",
      "2.443283796310425\n",
      "2.5678651332855225\n",
      "2.4914209842681885\n",
      "2.4786462783813477\n",
      "2.45135235786438\n",
      "2.5097291469573975\n",
      "2.551025152206421\n",
      "2.4697892665863037\n",
      "2.598670721054077\n",
      "2.51947021484375\n",
      "2.4426443576812744\n",
      "2.515302896499634\n",
      "2.3950464725494385\n",
      "2.620820999145508\n",
      "2.5567355155944824\n",
      "2.4123940467834473\n",
      "2.306281566619873\n",
      "2.451340436935425\n",
      "2.547844409942627\n",
      "2.513976812362671\n",
      "2.5781819820404053\n",
      "2.6172168254852295\n",
      "2.5822947025299072\n",
      "2.54801607131958\n",
      "2.4307546615600586\n",
      "2.30796217918396\n",
      "2.473203659057617\n",
      "2.4486215114593506\n",
      "2.4547064304351807\n",
      "2.5535953044891357\n",
      "2.477703809738159\n",
      "2.477525472640991\n",
      "2.3664941787719727\n",
      "2.526564121246338\n",
      "2.605548620223999\n",
      "2.430795431137085\n",
      "2.387800455093384\n",
      "2.332578420639038\n",
      "2.3826160430908203\n",
      "2.4171395301818848\n",
      "2.480119228363037\n",
      "2.4022512435913086\n",
      "2.4717416763305664\n",
      "2.4633612632751465\n",
      "2.423374652862549\n",
      "2.5458824634552\n",
      "2.4279510974884033\n",
      "2.4846248626708984\n",
      "2.4263927936553955\n",
      "2.4681856632232666\n",
      "2.4378044605255127\n",
      "2.3973777294158936\n",
      "2.5310118198394775\n",
      "2.502228021621704\n",
      "2.678727865219116\n",
      "2.337907314300537\n",
      "2.433278799057007\n",
      "2.4401214122772217\n",
      "2.55047345161438\n",
      "2.5885281562805176\n",
      "2.525499105453491\n",
      "2.517958879470825\n",
      "2.4392685890197754\n",
      "2.555591344833374\n",
      "2.4341647624969482\n",
      "2.462514638900757\n",
      "2.507323741912842\n",
      "2.513962984085083\n",
      "2.5718348026275635\n",
      "2.4671058654785156\n",
      "2.3538923263549805\n",
      "2.509246826171875\n",
      "2.343451976776123\n",
      "2.5019617080688477\n",
      "2.4164586067199707\n",
      "2.565659761428833\n",
      "2.544246196746826\n",
      "2.538663625717163\n",
      "2.532613515853882\n",
      "2.4011309146881104\n",
      "2.4152398109436035\n",
      "2.841230630874634\n",
      "2.4362502098083496\n",
      "2.347202777862549\n",
      "2.488706588745117\n",
      "2.5578866004943848\n",
      "2.3328566551208496\n",
      "2.5094149112701416\n",
      "2.3825154304504395\n",
      "2.281517505645752\n",
      "2.6122663021087646\n",
      "2.396453619003296\n",
      "2.434377908706665\n",
      "2.503438949584961\n",
      "2.439932346343994\n",
      "2.4222588539123535\n",
      "2.4038054943084717\n",
      "2.5952048301696777\n",
      "2.480177164077759\n",
      "2.485435962677002\n",
      "2.6407580375671387\n",
      "2.4870834350585938\n",
      "2.4804794788360596\n",
      "2.4861667156219482\n",
      "2.3935508728027344\n",
      "2.3897528648376465\n",
      "2.4845058917999268\n",
      "2.4535562992095947\n",
      "2.3605146408081055\n",
      "2.437373399734497\n",
      "2.539511203765869\n",
      "2.4984488487243652\n",
      "2.496250629425049\n",
      "2.472909688949585\n",
      "2.4545772075653076\n",
      "2.4019322395324707\n",
      "2.405292510986328\n",
      "2.5116121768951416\n",
      "2.542365789413452\n",
      "2.617577075958252\n",
      "2.3762307167053223\n",
      "2.4008615016937256\n",
      "2.400160789489746\n",
      "2.5117523670196533\n",
      "2.5354483127593994\n",
      "2.403102159500122\n",
      "2.48622465133667\n",
      "2.538806438446045\n",
      "2.521000862121582\n",
      "2.3461897373199463\n",
      "2.5795950889587402\n",
      "2.586405038833618\n",
      "2.488431453704834\n",
      "2.5110435485839844\n",
      "2.519167900085449\n",
      "2.5351104736328125\n",
      "2.4870333671569824\n",
      "2.4099016189575195\n",
      "2.4510159492492676\n",
      "2.405971050262451\n",
      "2.4020159244537354\n",
      "2.5119073390960693\n",
      "2.470731735229492\n",
      "2.3381507396698\n",
      "2.446202039718628\n",
      "2.3337223529815674\n",
      "2.3582019805908203\n",
      "2.3374574184417725\n",
      "2.6486527919769287\n",
      "2.5884287357330322\n",
      "2.858166217803955\n",
      "2.5120620727539062\n",
      "2.5625383853912354\n",
      "2.60601544380188\n",
      "2.5947582721710205\n",
      "2.4416134357452393\n",
      "2.496947765350342\n",
      "2.4836678504943848\n",
      "2.389220952987671\n",
      "2.3976693153381348\n",
      "2.523076295852661\n",
      "2.369997978210449\n",
      "2.3558900356292725\n",
      "2.3932034969329834\n",
      "2.40566349029541\n",
      "2.6386520862579346\n",
      "2.4185471534729004\n",
      "2.488050937652588\n",
      "2.5250563621520996\n",
      "2.5379819869995117\n",
      "2.4515159130096436\n",
      "2.55775785446167\n",
      "2.4304330348968506\n",
      "2.5739855766296387\n",
      "2.511774778366089\n",
      "2.4658701419830322\n",
      "2.514072895050049\n",
      "2.4439754486083984\n",
      "2.523585081100464\n",
      "2.5830237865448\n",
      "2.2823874950408936\n",
      "2.468384265899658\n",
      "2.424142360687256\n",
      "2.3807320594787598\n",
      "2.4758503437042236\n",
      "2.4419374465942383\n",
      "2.498164176940918\n",
      "2.3914859294891357\n",
      "2.479611873626709\n",
      "2.446589946746826\n",
      "2.333322525024414\n",
      "2.43462872505188\n",
      "2.6665635108947754\n",
      "2.5234909057617188\n",
      "2.361987352371216\n",
      "2.483233690261841\n",
      "2.463674545288086\n",
      "2.4779486656188965\n",
      "2.4939873218536377\n",
      "2.4627511501312256\n",
      "2.6135854721069336\n",
      "2.5001614093780518\n",
      "2.529960870742798\n",
      "2.333448886871338\n",
      "2.5204761028289795\n",
      "2.4951014518737793\n",
      "2.6169888973236084\n",
      "2.5066492557525635\n",
      "2.5100138187408447\n",
      "2.371896743774414\n",
      "2.446712017059326\n",
      "2.4744296073913574\n",
      "2.4021973609924316\n",
      "2.5928568840026855\n",
      "2.5689845085144043\n",
      "2.521794080734253\n",
      "2.376126766204834\n",
      "2.3745906352996826\n",
      "2.443716287612915\n",
      "2.498657464981079\n",
      "2.4549973011016846\n",
      "2.5880143642425537\n",
      "2.574836254119873\n",
      "2.472430944442749\n",
      "2.4332923889160156\n",
      "2.480670928955078\n",
      "2.5177600383758545\n",
      "2.3561081886291504\n",
      "2.392003059387207\n",
      "2.3556249141693115\n",
      "2.406409502029419\n",
      "2.4360098838806152\n",
      "2.4690651893615723\n",
      "2.370152235031128\n",
      "2.4172825813293457\n",
      "2.5007035732269287\n",
      "2.4311327934265137\n"
     ]
    }
   ],
   "source": [
    "# Pytorch optimizer\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.01)\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(1000):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Buceckierelsaru t IAREEroutersay.\n",
      "AD:\n",
      "Se m ane! ctcobsad, s IAse h mexadame, makisoung, hall-hithin p wate st--\n",
      "TUCLIfun met th hire onsthiapu cour chorlothay Mabe t VOLUKERUS:\n",
      "\n",
      "ABEY:\n",
      "Ans s har ug y'd it trr,\n",
      "I lay:\n",
      "EOMy athaveanghur amexeousss:\n",
      "F in\n",
      "\n",
      "Buru.\n",
      "Selum's\n",
      "IA:\n",
      "AUESTrisin. thed thmyo as ketoret?\n",
      "Yovethethellend:\n",
      "GAnd fo,\n",
      "CKI:\n",
      "DULird m wotintoupenow an,\n",
      "Un. houf,\n",
      "HELOLALod bery s. angupre; l We t lute, ts m re, drer f s thatheewisto burinoura s,\n",
      "Tono u t acou s, ingh cak r deftilselend wn\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x =  torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.3596, -0.9152],\n",
       "         [ 0.6258,  0.0255],\n",
       "         [ 0.9545,  0.0643],\n",
       "         [ 0.3612,  1.1679],\n",
       "         [-1.3499, -0.5102],\n",
       "         [ 0.2360, -0.2398],\n",
       "         [-0.9211,  1.5433]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range (T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "\n",
    "x[0], xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.3596, -0.9152],\n",
       "         [ 0.6258,  0.0255],\n",
       "         [ 0.9545,  0.0643],\n",
       "         [ 0.3612,  1.1679],\n",
       "         [-1.3499, -0.5102],\n",
       "         [ 0.2360, -0.2398],\n",
       "         [-0.9211,  1.5433]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "print(wei)\n",
    "xbow2 = wei @ x # wei initially is (T,T) and x is (B,T,C), Pytorch will add a dimension to wei to make it (1,T,T) and broadcast it to (B,T,T)\n",
    "torch.allclose(xbow, xbow2)\n",
    "x[0], xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n",
    "xbow[0], xbow3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x =  torch.randn(B,T,C)\n",
    "\n",
    "# single head to perform self-attention\n",
    "head_size = 16 # H\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # B,T,H\n",
    "q = query(x) # B,T,H\n",
    "v = value(x) # B,T,H\n",
    "wei = q @ k.transpose(-2,-1)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "#out = wei @ x\n",
    "oy = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "\n",
    "* Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "* There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "* In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "* \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "* \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0036,  0.0850,  1.0277],\n",
      "        [-0.3999,  0.8997,  0.2513],\n",
      "        [ 0.0527,  2.4278, -1.3424]])\n",
      "tensor([[ 0.0850,  1.0277],\n",
      "        [ 0.8997,  0.2513],\n",
      "        [ 2.4278, -1.3424]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3,3)\n",
    "print(a)\n",
    "print(a[:,-2:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
